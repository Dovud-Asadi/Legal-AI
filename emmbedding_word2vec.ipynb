{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1**"
      ],
      "metadata": {
        "id": "EWmKz_Y_LUSb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Libraries**"
      ],
      "metadata": {
        "id": "JqOjredmJB2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import gdown\n",
        "from gensim.models import Word2Vec\n",
        "import multiprocessing\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import ast"
      ],
      "metadata": {
        "id": "UYq_ZVwIJM9_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 147
        },
        "id": "ZF3DQjGtEJH3",
        "outputId": "2fe21973-3d5a-404c-efc3-11f94e1cba8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1rr8qaWBra7JtXFd5AcrtB3H147DtQ7VE\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1rr8qaWBra7JtXFd5AcrtB3H147DtQ7VE&confirm=t&uuid=37a284b7-f99c-4e03-8605-56c7e9ce4a05\n",
            "To: /content/gathered.json\n",
            "100%|██████████| 382M/382M [00:10<00:00, 37.7MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'gathered.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ],
      "source": [
        "import gdown\n",
        "file_id = '1rr8qaWBra7JtXFd5AcrtB3H147DtQ7VE'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "gdown.download(url, quiet=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing text**"
      ],
      "metadata": {
        "id": "PYE1TEC8JE3z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/content/gathered.json')"
      ],
      "metadata": {
        "id": "iY1Ttf72GCZW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_remove = 'Hujjatga taklif yuborish Audioni tinglash'\n",
        "\n",
        "for i in range(len(df.related_texts)):\n",
        "    for j in range(len(df.related_texts[i])):\n",
        "        df.related_texts[i][j] = df.related_texts[i][j].replace(text_to_remove, \"\")"
      ],
      "metadata": {
        "id": "yUy3u4AMIPNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess(text_list):\n",
        "    processed_texts = []\n",
        "    for text in text_list:\n",
        "        text = text.lower()\n",
        "        text = re.sub(r'\\W+', ' ', text)\n",
        "        tokens = text.split()\n",
        "        processed_texts.append(tokens)\n",
        "    return processed_texts\n",
        "\n",
        "df.loc[:, 'related_texts'] = df['related_texts'].apply(preprocess)"
      ],
      "metadata": {
        "id": "dhYyzZwVKU3p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "processed_docs = df['related_texts'].tolist()"
      ],
      "metadata": {
        "id": "x6TA976WLJq3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "flattened_docs = [word for sublist in processed_docs for word in sublist]"
      ],
      "metadata": {
        "id": "EyfbafQOLNoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Word2Vec**"
      ],
      "metadata": {
        "id": "vH7_ty85JIJg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec(sentences=flattened_docs, vector_size=300, window=5, min_count=1, workers=multiprocessing.cpu_count())\n",
        "\n",
        "if '<OOV>' not in model.wv:\n",
        "    model.wv.add_vector('<OOV>', np.zeros(model.vector_size))\n",
        "\n",
        "model.save(\"uzbek_law_word2vec.model\")\n",
        "\n",
        "model = Word2Vec.load(\"uzbek_law_word2vec.model\")"
      ],
      "metadata": {
        "id": "5YdTosoEJhUB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Preprocessing text**"
      ],
      "metadata": {
        "id": "iCH61uqIJSok"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = Word2Vec.load(\"uzbek_law_word2vec.model\")\n",
        "\n",
        "def preprocess_data(df, model):\n",
        "    def contains_04_0(okoz_list):\n",
        "        return any('04.0' in item for item in okoz_list)\n",
        "\n",
        "    def filter_by_length(text_list):\n",
        "        return [text for text in text_list if len(text) > 4]\n",
        "\n",
        "    def keep_elements_starting_with_04(text_list):\n",
        "        return [element for element in text_list if element.startswith('04')]\n",
        "\n",
        "    def clean_okoz_text(text_list):\n",
        "        return [text.replace(\"04.00.00.00 Oila qonunchiligi /\", \"\").strip(\"[]\").strip() for text in text_list]\n",
        "\n",
        "    def process_okoz_text(text_list):\n",
        "        return [text.split('/')[0].strip() if '/' in text else text for text in text_list]\n",
        "\n",
        "    def has_minimum_elements(text_list, min_length=0):\n",
        "        return len(text_list) > min_length\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def remove_duplicates(text_list):\n",
        "        return list(set(text_list))\n",
        "\n",
        "    def remove_semicolons(text_list):\n",
        "        return [text.replace(';', '') for text in text_list]\n",
        "\n",
        "    def get_embedding(text, model, oov_token='<OOV>'):\n",
        "        tokens = text.split()\n",
        "        embeddings = []\n",
        "        for token in tokens:\n",
        "            if token in model.wv:\n",
        "                embeddings.append(model.wv[token])\n",
        "            else:\n",
        "                embeddings.append(model.wv[oov_token])\n",
        "        if embeddings:\n",
        "            return np.mean(embeddings, axis=0)\n",
        "        else:\n",
        "            return np.zeros(model.vector_size)\n",
        "\n",
        "    df = df[df['okoz_text'].apply(contains_04_0)]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(filter_by_length)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(keep_elements_starting_with_04)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(clean_okoz_text)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(process_okoz_text)\n",
        "    df = df[df['okoz_text'].apply(lambda x: has_minimum_elements(x))]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_duplicates)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_semicolons)\n",
        "    df = df[df['okoz_text'].apply(len) == 1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    data = [\n",
        "        {\"okoz_text\": df.loc[i, 'okoz_text'], \"related_texts\": related_text}\n",
        "        for i in range(len(df))\n",
        "        for related_text in df.loc[i, 'related_texts']\n",
        "    ]\n",
        "    json_string = json.dumps(data, ensure_ascii=False, indent=4)\n",
        "    with open('corrected.json', 'w', encoding='utf-8') as f:\n",
        "        f.write(json_string)\n",
        "\n",
        "    df = pd.read_json(\"corrected.json\")\n",
        "\n",
        "    df.loc[:, 'related_texts'] = df['related_texts'].apply(preprocess_text)\n",
        "\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "\n",
        "    label_to_numeric = {}\n",
        "    label_counter = 1\n",
        "\n",
        "    for label, count in class_counts.items():\n",
        "        if count < 1000:\n",
        "            label_to_numeric[label] = 0\n",
        "        else:\n",
        "            label_to_numeric[label] = label_counter\n",
        "            label_counter += 1\n",
        "\n",
        "    df.loc[:, 'label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    # Apply the custom Word2Vec model to get embeddings\n",
        "    df.loc[:, 'embeddings'] = df['related_texts'].apply(lambda text: get_embedding(text, model))\n",
        "\n",
        "    columns_to_drop = ['related_texts', 'okoz_text']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    return df, label_to_numeric\n",
        "\n",
        "df = pd.read_json(\"full_okoz.json\")\n",
        "df, label_to_numeric = preprocess_data(df, model)\n"
      ],
      "metadata": {
        "id": "UXuGFcpDIRBE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_model(df, batch_size):\n",
        "    X = np.array(df['embeddings'].tolist())\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = prep_model(df, 32)"
      ],
      "metadata": {
        "id": "UkFUX9nTOZEG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model Training**"
      ],
      "metadata": {
        "id": "jIBZT5mCJdDR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = 6\n",
        "input_dim = np.array(df['embeddings'][0]).shape[0]"
      ],
      "metadata": {
        "id": "gnwjhCZYOgn5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "XMkIkrtvOiVR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in test_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)  # Move data to GPU\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {avg_loss:.4f}\")\n",
        "    return accuracy, avg_loss"
      ],
      "metadata": {
        "id": "VLjWCdExOj2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 1 (Accuracy: 87.4%)**"
      ],
      "metadata": {
        "id": "KElS0EvrJkeY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassModel1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassModel1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.layer1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batch_norm2(self.layer2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "vsBKWc6FOlIm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MulticlassModel1(input_size=input_dim, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ],
      "metadata": {
        "id": "bsm2lc1rOnAf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer,scheduler, num_epochs=100)\n",
        "evaluate_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VhRY0J7eOoPD",
        "outputId": "1ea35b24-701c-4f89-db64-a4c0452df9e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 0.957749\n",
            "Epoch 11/100, Loss: 0.479379\n",
            "Epoch 21/100, Loss: 0.379683\n",
            "Epoch 31/100, Loss: 0.313936\n",
            "Epoch 41/100, Loss: 0.263371\n",
            "Epoch 51/100, Loss: 0.253834\n",
            "Epoch 61/100, Loss: 0.225275\n",
            "Epoch 71/100, Loss: 0.160433\n",
            "Epoch 81/100, Loss: 0.150791\n",
            "Epoch 91/100, Loss: 0.152734\n",
            "Training Complete\n",
            "Test Accuracy: 87.40%, Test Loss: 0.5379\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(87.40068104426788, 0.537924875691674)"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Model 2 (Accuracy: 84.9%)**"
      ],
      "metadata": {
        "id": "BF3iPMd4Jt4g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassModel2(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassModel2, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.bn2 = nn.BatchNorm1d(1024)\n",
        "        self.layer3 = nn.Linear(1024, 2048)\n",
        "        self.bn3 = nn.BatchNorm1d(2048)\n",
        "        self.layer4 = nn.Linear(2048, 1024)\n",
        "        self.bn4 = nn.BatchNorm1d(1024)\n",
        "        self.layer5 = nn.Linear(1024, 512)\n",
        "        self.bn5 = nn.BatchNorm1d(512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.elu = nn.ELU()\n",
        "        self.dropout1 = nn.Dropout(p=0.3)\n",
        "        self.dropout2 = nn.Dropout(p=0.4)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.elu(self.bn1(self.layer1(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.elu(self.bn2(self.layer2(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn3(self.layer3(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn4(self.layer4(x)))\n",
        "        x = self.dropout2(x)\n",
        "        x = self.elu(self.bn5(self.layer5(x)))\n",
        "        x = self.dropout1(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "8WUkaP-1PYaf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MulticlassModel2(input_size=input_dim, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "scheduler = ReduceLROnPlateau(optimizer, 'min')"
      ],
      "metadata": {
        "id": "u858e7BFPZfw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer,scheduler, num_epochs=31)\n",
        "evaluate_model(model, test_loader, criterion)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCVVPohFPa-p",
        "outputId": "f56cb44b-1277-44ca-fd4a-b445ec54391d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/31, Loss: 0.985254\n",
            "Epoch 11/31, Loss: 0.594767\n",
            "Epoch 21/31, Loss: 0.462706\n",
            "Epoch 31/31, Loss: 0.377728\n",
            "Training Complete\n",
            "Test Accuracy: 84.90%, Test Loss: 0.4525\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(84.90351872871737, 0.45248965185517576)"
            ]
          },
          "metadata": {},
          "execution_count": 48
        }
      ]
    }
  ]
}