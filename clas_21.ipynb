{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "JY8J_U_d5nEV",
        "6bnS8cQR_5G7"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Libraries**"
      ],
      "metadata": {
        "id": "JY8J_U_d5nEV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uAtHBNis4Qw6"
      },
      "outputs": [],
      "source": [
        "!pip install  sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import gdown\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from huggingface_hub import login\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import ast\n",
        "from torch.amp import autocast"
      ],
      "metadata": {
        "id": "j0nujUO15h2k"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token = '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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YjqQVFP_5mPZ",
        "outputId": "d822e647-84ab-4ae6-c131-976818dc40ab"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Files**"
      ],
      "metadata": {
        "id": "6bnS8cQR_5G7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1LoIkGczZJZVTz88_xJg3aBDWpALEqGla'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "gdown.download(url, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "id": "cavRyrux51gN",
        "outputId": "48ef9dd7-bce4-4161-a581-9d9e82a5f077"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1LoIkGczZJZVTz88_xJg3aBDWpALEqGla\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1LoIkGczZJZVTz88_xJg3aBDWpALEqGla&confirm=t&uuid=6c515004-80e7-4df7-be85-7a1fbbdedb49\n",
            "To: /content/umid.json\n",
            "100%|██████████| 381M/381M [00:13<00:00, 28.9MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'umid.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Tokenizer and Embedding**"
      ],
      "metadata": {
        "id": "H_LkQHGl_9em"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1dWP_krhq_jSZdxmYN4gCQXDpLfmAWN7l?usp=sharing', output='uzbek_xlm_roberta_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lTSurJ2L577t",
        "outputId": "901853c6-e55f-445f-cb8f-e9d9b13cb8b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB config.json\n",
            "Processing file 1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB\n",
            "To: /content/uzbek_xlm_roberta_model/config.json\n",
            "100%|██████████| 709/709 [00:00<00:00, 667kB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU\n",
            "From (redirected): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU&confirm=t&uuid=4aa675cb-f31f-44f5-930b-4e5c8ab3869b\n",
            "To: /content/uzbek_xlm_roberta_model/model.safetensors\n",
            "100%|██████████| 2.24G/2.24G [01:13<00:00, 30.5MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_model/config.json',\n",
              " 'uzbek_xlm_roberta_model/model.safetensors']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1UDLQbCEkdS5DWKokzl-NFqxHlBZ6MHK_?usp=sharing', output='uzbek_xlm_roberta_tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EI_Q8kZi6Fte",
        "outputId": "3f6600db-d75f-4c4b-b7ef-30e6c35bbfb2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG sentencepiece.bpe.model\n",
            "Processing file 1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb special_tokens_map.json\n",
            "Processing file 1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model\n",
            "100%|██████████| 5.07M/5.07M [00:00<00:00, 48.3MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/special_tokens_map.json\n",
            "100%|██████████| 280/280 [00:00<00:00, 1.14MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/tokenizer_config.json\n",
            "100%|██████████| 1.17k/1.17k [00:00<00:00, 3.76MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model',\n",
              " 'uzbek_xlm_roberta_tokenizer/special_tokens_map.json',\n",
              " 'uzbek_xlm_roberta_tokenizer/tokenizer_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/uzbek_xlm_roberta_model\"\n",
        "tokenizer_dir = \"/content/uzbek_xlm_roberta_tokenizer\"\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_dir)\n",
        "model_embedding = XLMRobertaModel.from_pretrained(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model_embedding.to(device)\n",
        "\n",
        "model_embedding.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZc253kz6KIN",
        "outputId": "c986e76c-79ce-41f2-8723-68c97fdcfd05"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /content/uzbek_xlm_roberta_model and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaModel(\n",
              "  (embeddings): XLMRobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): XLMRobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x XLMRobertaLayer(\n",
              "        (attention): XLMRobertaAttention(\n",
              "          (self): XLMRobertaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): XLMRobertaSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): XLMRobertaIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): XLMRobertaOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): XLMRobertaPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Data Preprocessing**"
      ],
      "metadata": {
        "id": "bActvCqaAJkT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_umiod = pd.read_json('/content/umid.json')"
      ],
      "metadata": {
        "id": "_KL-jViq6OX8"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_remove = 'Hujjatga taklif yuborish Audioni tinglash'\n",
        "\n",
        "for i in range(len(df_umiod.related_texts)):\n",
        "    for j in range(len(df_umiod.related_texts[i])):\n",
        "        df_umiod.related_texts[i][j] = df_umiod.related_texts[i][j].replace(text_to_remove, \"\")"
      ],
      "metadata": {
        "id": "v3eg_gFO6TP5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_okoz(df, number):\n",
        "    def contains_04_0(okoz_list):\n",
        "        return any(f'{number}.0' in item for item in okoz_list)#change\n",
        "\n",
        "    def filter_by_length(text_list):\n",
        "        return [text for text in text_list if len(text) > 4]\n",
        "\n",
        "    def keep_elements_starting_with_04(text_list):\n",
        "        return [element for element in text_list if element.startswith(f'{number}')] #change\n",
        "\n",
        "    def process_okoz_text(text_list):\n",
        "        return [text.split('/')[0].strip() if '/' in text else text for text in text_list]\n",
        "\n",
        "    def has_minimum_elements(text_list, min_length=0):\n",
        "        return len(text_list) > min_length\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def remove_duplicates(text_list):\n",
        "        return list(set(text_list))\n",
        "\n",
        "    def remove_semicolons(text_list):\n",
        "        return [text.replace(';', '') for text in text_list]\n",
        "\n",
        "    df = df[df['okoz_text'].apply(contains_04_0)]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(filter_by_length)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(keep_elements_starting_with_04)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(process_okoz_text)\n",
        "    df = df[df['okoz_text'].apply(lambda x: has_minimum_elements(x))]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_duplicates)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_semicolons)\n",
        "    df = df[df['okoz_text'].apply(len) == 1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda x: ' '.join(x))\n",
        "    return df"
      ],
      "metadata": {
        "id": "HOb3w-ry6YWp"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = preprocess_okoz(df_umiod, '01')\n",
        "df_2 = preprocess_okoz(df_umiod, '02')\n",
        "df_3 = preprocess_okoz(df_umiod, '03')\n",
        "df_4 = preprocess_okoz(df_umiod, '04')\n",
        "df_5 = preprocess_okoz(df_umiod, '05')\n",
        "df_6 = preprocess_okoz(df_umiod, '06')\n",
        "df_7 = preprocess_okoz(df_umiod, '07')\n",
        "df_8 = preprocess_okoz(df_umiod, '08')\n",
        "df_9 = preprocess_okoz(df_umiod, '09')\n",
        "df_10 = preprocess_okoz(df_umiod, '10')\n",
        "df_11 = preprocess_okoz(df_umiod, '11')\n",
        "df_12 = preprocess_okoz(df_umiod, '12')\n",
        "df_13 = preprocess_okoz(df_umiod, '13')\n",
        "df_14 = preprocess_okoz(df_umiod, '14')\n",
        "df_15 = preprocess_okoz(df_umiod, '15')\n",
        "df_16 = preprocess_okoz(df_umiod, '16')\n",
        "df_17 = preprocess_okoz(df_umiod, '17')\n",
        "df_18 = preprocess_okoz(df_umiod, '18')\n",
        "df_19 = preprocess_okoz(df_umiod, '19')\n",
        "df_20 = preprocess_okoz(df_umiod, '20')\n",
        "df_21 = preprocess_okoz(df_umiod, '21')"
      ],
      "metadata": {
        "id": "VYZfm_iL6lg0"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10,\n",
        "                    df_11, df_12, df_13, df_14, df_15, df_16, df_17, df_18, df_19,\n",
        "                    df_20, df_21], ignore_index=True)"
      ],
      "metadata": {
        "id": "t2r88pA56mLI"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df):\n",
        "\n",
        "  def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "\n",
        "  df['related_texts'] = df['related_texts'].apply(lambda x: ' '.join(x))\n",
        "  df['related_texts'] = df['related_texts'].apply(clean_individual_text)\n",
        "  return df"
      ],
      "metadata": {
        "id": "cR0V22oq6s3C"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = preprocess_text(df_all)"
      ],
      "metadata": {
        "id": "Evwk6q0c6v1v"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.okoz_text.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 774
        },
        "id": "0oft0-Fw6zMP",
        "outputId": "f923d784-c60e-432e-e4e5-bfb90c60653b"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "okoz_text\n",
              "09.00.00.00 Tadbirkorlik va xo‘jalik faoliyati                                                                                                                                                            13282\n",
              "01.00.00.00 Konstitutsiyaviy tuzum                                                                                                                                                                        10002\n",
              "07.00.00.00 Moliya va kredit to‘g‘risidagi qonunchilik. Bank faoliyati                                                                                                                                     9439\n",
              "02.00.00.00 Davlat boshqaruvi asoslari                                                                                                                                                                     7196\n",
              "21.00.00.00 O‘zgartirish va qo‘shimchalar kiritish bo‘yicha kompleks tusdagi hujjatlar                                                                                                                     6889\n",
              "03.00.00.00 Fuqarolik qonunchiligi                                                                                                                                                                         5713\n",
              "13.00.00.00 Ta’lim. Fan. Madaniyat                                                                                                                                                                         5305\n",
              "16.00.00.00 Xavfsizlik va huquq tartibot muhofazasi                                                                                                                                                        3795\n",
              "11.00.00.00 Atrof tabiiy muhit va tabiiy resurslar                                                                                                                                                         3592\n",
              "14.00.00.00 Sog‘liqni saqlash. Jismoniy tarbiya. Sport. Turizm                                                                                                                                             3041\n",
              "05.00.00.00 Mehnat va aholining bandligi to‘g‘risidagi qonunchilik                                                                                                                                         2926\n",
              "10.00.00.00 Tashqi iqtisodiy faoliyat. Bojxona ishi                                                                                                                                                        2576\n",
              "19.00.00.00 Xalqaro munosabatlar. Xalqaro huquq                                                                                                                                                            2496\n",
              "12.00.00.00 Axborot va axborotlashtirish                                                                                                                                                                   2091\n",
              "06.00.00.00 Ijtimoiy ta’minot va ijtimoiy sug‘urta to‘g‘risidagi qonunchilik. Ijtimoiy himoya                                                                                                              1819\n",
              "17.00.00.00 Odil sudlov                                                                                                                                                                                    1593\n",
              "08.00.00.00 Uy-joy qonunchiligi. Kommunal xo‘jalik                                                                                                                                                         1269\n",
              "18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari                                                                                                            1161\n",
              "15.00.00.00 Mudofaa                                                                                                                                                                                         804\n",
              "04.00.00.00 Oila qonunchiligi                                                                                                                                                                               605\n",
              "20.00.00.00 Kadrlar masalalari, mukofotlar, faxriy unvonlar, esdalik nishonlar, faxriy yorliqlar bilan taqdirlash, amnistiya va afv etish masalalari hamda fuqarolik berish bo‘yicha individual aktlar      110\n",
              "Name: count, dtype: int64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>okoz_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>09.00.00.00 Tadbirkorlik va xo‘jalik faoliyati</th>\n",
              "      <td>13282</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01.00.00.00 Konstitutsiyaviy tuzum</th>\n",
              "      <td>10002</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>07.00.00.00 Moliya va kredit to‘g‘risidagi qonunchilik. Bank faoliyati</th>\n",
              "      <td>9439</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02.00.00.00 Davlat boshqaruvi asoslari</th>\n",
              "      <td>7196</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21.00.00.00 O‘zgartirish va qo‘shimchalar kiritish bo‘yicha kompleks tusdagi hujjatlar</th>\n",
              "      <td>6889</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03.00.00.00 Fuqarolik qonunchiligi</th>\n",
              "      <td>5713</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13.00.00.00 Ta’lim. Fan. Madaniyat</th>\n",
              "      <td>5305</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16.00.00.00 Xavfsizlik va huquq tartibot muhofazasi</th>\n",
              "      <td>3795</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11.00.00.00 Atrof tabiiy muhit va tabiiy resurslar</th>\n",
              "      <td>3592</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14.00.00.00 Sog‘liqni saqlash. Jismoniy tarbiya. Sport. Turizm</th>\n",
              "      <td>3041</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>05.00.00.00 Mehnat va aholining bandligi to‘g‘risidagi qonunchilik</th>\n",
              "      <td>2926</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.00.00.00 Tashqi iqtisodiy faoliyat. Bojxona ishi</th>\n",
              "      <td>2576</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19.00.00.00 Xalqaro munosabatlar. Xalqaro huquq</th>\n",
              "      <td>2496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12.00.00.00 Axborot va axborotlashtirish</th>\n",
              "      <td>2091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>06.00.00.00 Ijtimoiy ta’minot va ijtimoiy sug‘urta to‘g‘risidagi qonunchilik. Ijtimoiy himoya</th>\n",
              "      <td>1819</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17.00.00.00 Odil sudlov</th>\n",
              "      <td>1593</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08.00.00.00 Uy-joy qonunchiligi. Kommunal xo‘jalik</th>\n",
              "      <td>1269</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari</th>\n",
              "      <td>1161</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.00.00.00 Mudofaa</th>\n",
              "      <td>804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>04.00.00.00 Oila qonunchiligi</th>\n",
              "      <td>605</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.00.00.00 Kadrlar masalalari, mukofotlar, faxriy unvonlar, esdalik nishonlar, faxriy yorliqlar bilan taqdirlash, amnistiya va afv etish masalalari hamda fuqarolik berish bo‘yicha individual aktlar</th>\n",
              "      <td>110</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> int64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Embedding**"
      ],
      "metadata": {
        "id": "bbt8LmwpRGGk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_embedd(df, model, tokenizer, device, batch_size=16):\n",
        "    def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    # Embed documents in batches without mixed precision\n",
        "    def embed_document(df, model, tokenizer, device, batch_size=16):\n",
        "        embeddings_list = []\n",
        "        texts = df['related_texts'].apply(clean_individual_text).tolist()\n",
        "\n",
        "        # Tokenize all the texts\n",
        "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=256)  # Adjust max_length for speed\n",
        "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                embeddings = outputs.last_hidden_state\n",
        "                doc_embeddings = torch.mean(embeddings, dim=1)  # Take mean of token embeddings for document embedding\n",
        "\n",
        "            embeddings_list.extend(doc_embeddings.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "\n",
        "        df['embeddings'] = embeddings_list\n",
        "        return df\n",
        "\n",
        "    # Convert 'okoz_text' into numeric labels\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "\n",
        "    label_to_numeric = {}\n",
        "    label_counter = 1\n",
        "\n",
        "    # Map labels based on frequency\n",
        "    for label, count in class_counts.items():\n",
        "        if count <= 1000:\n",
        "            label_to_numeric[label] = 0  # Treat less frequent labels as class 0\n",
        "        else:\n",
        "            label_to_numeric[label] = label_counter\n",
        "            label_counter += 1\n",
        "\n",
        "    df['label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    # Get embeddings\n",
        "    df = embed_document(df, model, tokenizer, device, batch_size)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = ['okoz_text']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    return df, label_to_numeric"
      ],
      "metadata": {
        "id": "iR6hX1F27NJy"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_embedd(df, model, tokenizer, device, batch_size=16, max_length=256):\n",
        "    def clean_texts(df):\n",
        "        df['cleaned_text'] = df['related_texts'].str.lower() \\\n",
        "            .str.replace('‘', \"'\").str.replace('’', \"'\").str.replace('`', \"'\") \\\n",
        "            .str.replace(r'[^a-z\\.\\'\\s]', '', regex=True) \\\n",
        "            .str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
        "        return df\n",
        "\n",
        "    def embed_document(df, model, tokenizer, device, batch_size=16, max_length=256):\n",
        "        texts = df['cleaned_text'].tolist()\n",
        "\n",
        "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
        "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, num_workers=4)  # Use multiple workers for faster loading\n",
        "\n",
        "        embeddings_list = []\n",
        "        model.to(device)\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "            embeddings_list.extend(embeddings.cpu().numpy())\n",
        "\n",
        "        df['embeddings'] = embeddings_list\n",
        "        return df\n",
        "\n",
        "    # Clean text outside the embedding loop\n",
        "    df = clean_texts(df)\n",
        "\n",
        "    # Convert 'okoz_text' into numeric labels\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "    label_to_numeric = {label: (i+1 if count > 1000 else 0) for i, (label, count) in enumerate(class_counts.items())}\n",
        "    df['label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    # Embed documents\n",
        "    df = embed_document(df, model, tokenizer, device, batch_size, max_length)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(columns=['okoz_text'])\n",
        "\n",
        "    return df, label_to_numeric"
      ],
      "metadata": {
        "id": "hpPUH52hTdTT"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_embedd(df, model, tokenizer, device, batch_size=64, max_length=256):\n",
        "    def clean_texts(df):\n",
        "        df['cleaned_text'] = df['related_texts']\n",
        "        return df\n",
        "\n",
        "    def embed_document(df, model, tokenizer, device, batch_size=64, max_length=256):\n",
        "        texts = df['cleaned_text'].tolist()\n",
        "\n",
        "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=max_length)\n",
        "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size, pin_memory=True, num_workers=4)  # Enable pin_memory\n",
        "\n",
        "        embeddings_list = []\n",
        "        model.to(device)\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [t.to(device, non_blocking=True) for t in batch]  # Use non_blocking transfers\n",
        "            with torch.cuda.amp.autocast():  # Mixed precision\n",
        "                with torch.no_grad():\n",
        "                    outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                    embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "            embeddings_list.extend(embeddings.cpu().numpy())\n",
        "\n",
        "        df['embeddings'] = embeddings_list\n",
        "        return df\n",
        "\n",
        "    # Clean text outside the embedding loop\n",
        "    df = clean_texts(df)\n",
        "\n",
        "    # Convert 'okoz_text' into numeric labels\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "    label_to_numeric = {label: (i+1 if count > 1000 else 0) for i, (label, count) in enumerate(class_counts.items())}\n",
        "    df['label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    # Embed documents with higher batch size\n",
        "    df = embed_document(df, model, tokenizer, device, batch_size, max_length)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    df = df.drop(columns=['okoz_text'])\n",
        "\n",
        "    # Check GPU usage\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(device) / 1024**3:.2f} GB\")\n",
        "    print(f\"Memory reserved: {torch.cuda.memory_reserved(device) / 1024**3:.2f} GB\")\n",
        "\n",
        "    return df, label_to_numeric\n"
      ],
      "metadata": {
        "id": "lAKMgfTzV6hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedded, label_to_numeric = preprocess_data_embedd(df_new, model, tokenizer, device)"
      ],
      "metadata": {
        "id": "R0Nak-8L7fea",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9084843f-d5fa-47f4-baee-efb998458c2d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "<ipython-input-18-5f0e1622863a>:20: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():  # Mixed precision\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model Prep**"
      ],
      "metadata": {
        "id": "cErl58y0XSnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_model(df, batch_size):\n",
        "    X = np.array(df['embeddings'].tolist())\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "eVbEdh9i7he-"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = prep_model(df_embedded, 32)"
      ],
      "metadata": {
        "id": "viAuipoj8fkw"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = df_embedded['label'].nunique()\n",
        "input_dim = np.array(df_embedded['embeddings'][0]).shape[0]"
      ],
      "metadata": {
        "id": "zyX62PRGXq7x"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "IsvqFUA-Xu8Y"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, df):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    incorrect_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            # incorrect_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "            # for idx in incorrect_indices:\n",
        "            #     original_idx = i * test_loader.batch_size + idx.item()\n",
        "            #     text = df.iloc[original_idx]['related_texts']\n",
        "            #     true_label = labels[idx].item()\n",
        "            #     predicted_label = predicted[idx].item()\n",
        "            #     incorrect_predictions.append((text, true_label, predicted_label))\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    # if incorrect_predictions:\n",
        "    #     print(\"\\nIncorrect Predictions:\")\n",
        "    #     for text, true_label, predicted_label in incorrect_predictions:\n",
        "    #         print(f\"Text: {text}\")\n",
        "    #         print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\\n\")\n",
        "\n",
        "    return accuracy, avg_loss, incorrect_predictions"
      ],
      "metadata": {
        "id": "MxFt0B__X17U"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Model 1**"
      ],
      "metadata": {
        "id": "qfJZ5oZAX8EO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassModel1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassModel1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.layer1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batch_norm2(self.layer2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "c3KYfoUtYGU0"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MulticlassModel1(input_size=input_dim, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ],
      "metadata": {
        "id": "96BqESqoX7cm"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer,scheduler, num_epochs=100)\n",
        "accuracy, avg_loss, incorrect_predictions = evaluate_model(model, test_loader, criterion, df_embedded)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3fLLWoibYFE_",
        "outputId": "47dea54f-e385-4689-eaa4-6f330d2f2361"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100, Loss: 1.898570\n",
            "Epoch 11/100, Loss: 1.495120\n",
            "Epoch 21/100, Loss: 1.407473\n",
            "Epoch 31/100, Loss: 1.347501\n",
            "Epoch 41/100, Loss: 1.299845\n",
            "Epoch 51/100, Loss: 1.268829\n",
            "Epoch 61/100, Loss: 1.235550\n",
            "Epoch 71/100, Loss: 1.215521\n",
            "Epoch 81/100, Loss: 1.188098\n",
            "Epoch 91/100, Loss: 1.173119\n",
            "Training Complete\n",
            "Test Accuracy: 51.44%, Test Loss: 1.4287\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Enter Text\n",
        "text = \"O‘zbekiston Respublikasi Vazirlar Mahkamasi: davlat hisobidan yuridik yordam ko‘rsatish sohasida yagona davlat siyosati amalga oshirilishini ta’minlaydi; davlat hisobidan yuridik yordam ko‘rsatish sohasidagi davlat dasturlarini tasdiqlaydi va ularning amalga oshirilishini ta’minlaydi; advokatlar tomonidan davlat hisobidan ko‘rsatilgan yuridik yordam uchun haq to‘lash miqdori va tartibini belgilaydi; davlat hisobidan yuridik yordam ko‘rsatish sohasidagi normativ-huquqiy hujjatlarni o‘z vakolatlari doirasida qabul qiladi.\" #@param {type:\"string\"}\n"
      ],
      "metadata": {
        "id": "wzTfaxVRh_Ai"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # prompt: now i have model now create text and show in this 3 class does this text belong\n",
        "\n",
        "# #@title Enter Text\n",
        "# text = \"1. 5-bandning beshinchi xatboshisidagi “Vazirlar Mahkamasining 2022-yil 25-fevraldagi 89-son qarori bilan tasdiqlangan O‘limni qayd qilish bilan bog‘liq kompozit davlat xizmatlarini ko‘rsatishning ma’muriy reglamentiga” degan so‘zlar “Vazirlar Mahkamasining 2023-yil 20-oktabrdagi 550-son qarori bilan tasdiqlangan O‘limni qayd qilish bilan bog‘liq kompozit davlat xizmatlarini ko‘rsatishning ma’muriy reglamentiga” degan so‘zlar bilan almashtirilsin. 2. 11-bandning uchinchi xatboshisi quyidagi tahrirda bayon etilsin: “Vazirlar Mahkamasining 2023-yil 20-oktabrdagi 550-son qarori bilan tasdiqlangan Fuqarolik holati dalolatnomalarini qayd etish qoidalarining 4-bandiga asosan FHDY organlari, O‘zbekiston Respublikasining konsullik muassasalari va fuqarolar yig‘inlari fuqarolik holati dalolatnomalarining yozuv blanklari va boshqa hujjatlar bilan Vazirlik, shuningdek O‘zbekiston Respublikasi Tashqi ishlar vazirligi tomonidan ta’minlanadi.”.\" #@param {type:\"string\"}\n",
        "\n",
        "# def predict_class(text, model, tokenizer, label_to_numeric, device):\n",
        "#     \"\"\"\n",
        "#     Predicts the class of the given text using the trained model.\n",
        "\n",
        "#     Args:\n",
        "#         text: The input text.\n",
        "#         model: The trained model.\n",
        "#         tokenizer: The tokenizer used for the model.\n",
        "#         label_to_numeric: A dictionary mapping labels to numeric values.\n",
        "#         device: The device to use for computation (CPU or GPU).\n",
        "\n",
        "#     Returns:\n",
        "#         A list of top 3 predicted classes with their probabilities.\n",
        "#     \"\"\"\n",
        "\n",
        "#     # Preprocess the text\n",
        "#     cleaned_text = text.lower()\n",
        "#     cleaned_text = cleaned_text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "#     cleaned_text = re.sub(r'[^a-z\\.\\'\\s]', '', cleaned_text)\n",
        "#     cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "#     # Tokenize the text\n",
        "#     inputs = tokenizer(cleaned_text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
        "#     input_ids = inputs['input_ids'].to(device)\n",
        "#     attention_mask = inputs['attention_mask'].to(device)\n",
        "\n",
        "#     # Get the embeddings\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(input_ids, attention_mask=attention_mask)\n",
        "#         embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "#     # Predict the class\n",
        "#     with torch.no_grad():\n",
        "#         model.eval()\n",
        "#         outputs = model(embeddings)\n",
        "#         probabilities = F.softmax(outputs, dim=1)\n",
        "\n",
        "#     # Get the top 3 predictions\n",
        "#     top3_probs, top3_indices = torch.topk(probabilities, 3)\n",
        "\n",
        "#     # Map numeric labels back to original labels\n",
        "#     numeric_to_label = {v: k for k, v in label_to_numeric.items()}\n",
        "#     top3_predictions = [(numeric_to_label[idx.item()], prob.item()) for idx, prob in zip(top3_indices[0], top3_probs[0])]\n",
        "\n",
        "#     return top3_predictions\n",
        "\n",
        "# # Predict the class\n",
        "# predictions = predict_class(text, model, tokenizer, label_to_numeric, device)\n",
        "\n",
        "# # Print the predictions\n",
        "# print(\"Top 3 Predictions:\")\n",
        "# for label, prob in predictions:\n",
        "#     print(f\"Class: {label}, Probability: {prob:.4f}\")\n"
      ],
      "metadata": {
        "id": "rz9VuSl4fW4l"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Required imports\n",
        "import torch\n",
        "import re\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Prediction function\n",
        "def predict_class(text, model, tokenizer, label_to_numeric, device):\n",
        "    \"\"\"\n",
        "    Predicts the class of the given text using the trained model.\n",
        "\n",
        "    Args:\n",
        "        text: The input text.\n",
        "        model: The trained model.\n",
        "        tokenizer: The tokenizer used for the model.\n",
        "        label_to_numeric: A dictionary mapping labels to numeric values.\n",
        "        device: The device to use for computation (CPU or GPU).\n",
        "\n",
        "    Returns:\n",
        "        A list of top 3 predicted classes with their probabilities.\n",
        "    \"\"\"\n",
        "\n",
        "    # Preprocess the text\n",
        "    cleaned_text = text.lower()\n",
        "    cleaned_text = cleaned_text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "    cleaned_text = re.sub(r'[^a-z\\.\\'\\s]', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r'\\s+', ' ', cleaned_text).strip()\n",
        "\n",
        "    # Tokenize the text\n",
        "    inputs = tokenizer(cleaned_text, return_tensors='pt', truncation=True, padding=True, max_length=256)\n",
        "\n",
        "    # Move inputs to the same device as the model\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Get the embeddings\n",
        "    with torch.no_grad():\n",
        "        # Pass input_ids and attention mask to the model\n",
        "        outputs = model_embedding(**inputs)\n",
        "        embeddings = torch.mean(outputs.last_hidden_state, dim=1)\n",
        "\n",
        "    # Predict the class using the embedding as input to the classifier\n",
        "    with torch.no_grad():\n",
        "        model.eval()\n",
        "        logits = model(embeddings)  # Your classifier model expects the embeddings as input\n",
        "        probabilities = F.softmax(logits, dim=1)\n",
        "\n",
        "    # Get the top 3 predictions\n",
        "    top3_probs, top3_indices = torch.topk(probabilities, 3)\n",
        "\n",
        "    # Map numeric labels back to original labels\n",
        "    numeric_to_label = {v: k for k, v in label_to_numeric.items()}\n",
        "    top3_predictions = [(numeric_to_label[idx.item()], prob.item()) for idx, prob in zip(top3_indices[0], top3_probs[0])]\n",
        "\n",
        "    return top3_predictions\n",
        "\n",
        "# Example variables (you need to have these defined properly in your environment)\n",
        "# text = \"Your input text here\"\n",
        "# model = your_trained_model\n",
        "# tokenizer = your_tokenizer\n",
        "# label_to_numeric = {'class1': 0, 'class2': 1, 'class3': 2}  # Example\n",
        "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Call the function to make predictions\n",
        "predictions = predict_class(text, model, tokenizer, label_to_numeric, device)\n",
        "\n",
        "# Print the predictions\n",
        "print(\"Top 3 Predictions:\")\n",
        "for label, prob in predictions:\n",
        "    print(f\"Class: {label}, Probability: {prob:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94yBVxeqh7F4",
        "outputId": "0c3ddd94-8ef4-4691-d355-761cda1b1e10"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top 3 Predictions:\n",
            "Class: 18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari, Probability: 0.6200\n",
            "Class: 02.00.00.00 Davlat boshqaruvi asoslari, Probability: 0.2679\n",
            "Class: 01.00.00.00 Konstitutsiyaviy tuzum, Probability: 0.0333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import XLMRobertaModel # Import the XLMRobertaModel\n",
        "\n",
        "class MulticlassModel1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassModel1, self).__init__()\n",
        "        self.transformer = XLMRobertaModel.from_pretrained('xlm-roberta-base') # Initialize XLMRoberta\n",
        "        self.layer1 = nn.Linear(input_size, 512) # This layer should take the transformer output size as input\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask): # Modify the forward method to accept transformer inputs\n",
        "        x = self.transformer(input_ids=input_ids, attention_mask=attention_mask).last_hidden_state # Get the transformer output\n",
        "        x = torch.mean(x, dim=1) # Average the output across tokens\n",
        "        x = self.relu(self.batch_norm1(self.layer1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batch_norm2(self.layer2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "G7-JR59ghR4z"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}