{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "T8tg96vVvvIL"
      },
      "outputs": [],
      "source": [
        "!pip install  sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import gdown\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from huggingface_hub import login\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import ast\n",
        "from torch.amp import autocast"
      ],
      "metadata": {
        "id": "fp7M5uYuv6lb"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token = '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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GsBkv5-3v75j",
        "outputId": "41187bef-f21f-4904-c639-d129c4aae15b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '1LoIkGczZJZVTz88_xJg3aBDWpALEqGla'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "gdown.download(url, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "id": "Xpwt2cbrv_X1",
        "outputId": "ac94bc99-a251-498e-fe1a-6bcf067cc439"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?export=download&id=1LoIkGczZJZVTz88_xJg3aBDWpALEqGla\n",
            "From (redirected): https://drive.google.com/uc?export=download&id=1LoIkGczZJZVTz88_xJg3aBDWpALEqGla&confirm=t&uuid=77ee9bfc-56a6-41c4-8555-edc3781369b5\n",
            "To: /content/umid.json\n",
            "100%|██████████| 381M/381M [00:08<00:00, 45.8MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'umid.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1dWP_krhq_jSZdxmYN4gCQXDpLfmAWN7l?usp=sharing', output='uzbek_xlm_roberta_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-My5AI5XwA18",
        "outputId": "b61b45f0-470a-4de9-99cf-020bfedc3285"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB config.json\n",
            "Processing file 1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB\n",
            "To: /content/uzbek_xlm_roberta_model/config.json\n",
            "100%|██████████| 709/709 [00:00<00:00, 2.47MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU\n",
            "From (redirected): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU&confirm=t&uuid=09fd6fcb-1def-4de6-b83e-54417b7d7896\n",
            "To: /content/uzbek_xlm_roberta_model/model.safetensors\n",
            "100%|██████████| 2.24G/2.24G [00:50<00:00, 44.0MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_model/config.json',\n",
              " 'uzbek_xlm_roberta_model/model.safetensors']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1UDLQbCEkdS5DWKokzl-NFqxHlBZ6MHK_?usp=sharing', output='uzbek_xlm_roberta_tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jY1MjnKuwZ5J",
        "outputId": "409a3a51-745b-4c4c-f3df-6d86899bd143"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG sentencepiece.bpe.model\n",
            "Processing file 1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb special_tokens_map.json\n",
            "Processing file 1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model\n",
            "100%|██████████| 5.07M/5.07M [00:00<00:00, 28.7MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/special_tokens_map.json\n",
            "100%|██████████| 280/280 [00:00<00:00, 813kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/tokenizer_config.json\n",
            "100%|██████████| 1.17k/1.17k [00:00<00:00, 4.11MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model',\n",
              " 'uzbek_xlm_roberta_tokenizer/special_tokens_map.json',\n",
              " 'uzbek_xlm_roberta_tokenizer/tokenizer_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/uzbek_xlm_roberta_model\"\n",
        "tokenizer_dir = \"/content/uzbek_xlm_roberta_tokenizer\"\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_dir)\n",
        "model = XLMRobertaModel.from_pretrained(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U63K2EvswD_o",
        "outputId": "a0e32c4e-7402-4440-e95e-f8572d814611"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /content/uzbek_xlm_roberta_model and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaModel(\n",
              "  (embeddings): XLMRobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): XLMRobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x XLMRobertaLayer(\n",
              "        (attention): XLMRobertaAttention(\n",
              "          (self): XLMRobertaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): XLMRobertaSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): XLMRobertaIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): XLMRobertaOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): XLMRobertaPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_umiod = pd.read_json('/content/umid.json')"
      ],
      "metadata": {
        "id": "Xdc3gBd1wHsE"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_remove = 'Hujjatga taklif yuborish Audioni tinglash'\n",
        "\n",
        "for i in range(len(df_umiod.related_texts)):\n",
        "    for j in range(len(df_umiod.related_texts[i])):\n",
        "        df_umiod.related_texts[i][j] = df_umiod.related_texts[i][j].replace(text_to_remove, \"\")"
      ],
      "metadata": {
        "id": "HcAiTuvjxZyN"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_okoz(df, number):\n",
        "    def contains_04_0(okoz_list):\n",
        "        return any(f'{number}.0' in item for item in okoz_list)#change\n",
        "\n",
        "    def filter_by_length(text_list):\n",
        "        return [text for text in text_list if len(text) > 4]\n",
        "\n",
        "    def keep_elements_starting_with_04(text_list):\n",
        "        return [element for element in text_list if element.startswith(f'{number}')] #change\n",
        "\n",
        "    def process_okoz_text(text_list):\n",
        "        return [text.split('/')[0].strip() if '/' in text else text for text in text_list]\n",
        "\n",
        "    def has_minimum_elements(text_list, min_length=0):\n",
        "        return len(text_list) > min_length\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def remove_duplicates(text_list):\n",
        "        return list(set(text_list))\n",
        "\n",
        "    def remove_semicolons(text_list):\n",
        "        return [text.replace(';', '') for text in text_list]\n",
        "\n",
        "    df = df[df['okoz_text'].apply(contains_04_0)]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(filter_by_length)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(keep_elements_starting_with_04)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(process_okoz_text)\n",
        "    df = df[df['okoz_text'].apply(lambda x: has_minimum_elements(x))]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_duplicates)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_semicolons)\n",
        "    df = df[df['okoz_text'].apply(len) == 1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda x: ' '.join(x))\n",
        "    return df"
      ],
      "metadata": {
        "id": "ezGtkQiSwIy6"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = preprocess_okoz(df_umiod, '01')\n",
        "df_2 = preprocess_okoz(df_umiod, '02')\n",
        "df_3 = preprocess_okoz(df_umiod, '03')\n",
        "df_4 = preprocess_okoz(df_umiod, '04')\n",
        "df_5 = preprocess_okoz(df_umiod, '05')\n",
        "df_6 = preprocess_okoz(df_umiod, '06')\n",
        "df_7 = preprocess_okoz(df_umiod, '07')\n",
        "df_8 = preprocess_okoz(df_umiod, '08')\n",
        "df_9 = preprocess_okoz(df_umiod, '09')\n",
        "df_10 = preprocess_okoz(df_umiod, '10')\n",
        "df_11 = preprocess_okoz(df_umiod, '11')\n",
        "df_12 = preprocess_okoz(df_umiod, '12')\n",
        "df_13 = preprocess_okoz(df_umiod, '13')\n",
        "df_14 = preprocess_okoz(df_umiod, '14')\n",
        "df_15 = preprocess_okoz(df_umiod, '15')\n",
        "df_16 = preprocess_okoz(df_umiod, '16')\n",
        "df_17 = preprocess_okoz(df_umiod, '17')\n",
        "df_18 = preprocess_okoz(df_umiod, '18')\n",
        "df_19 = preprocess_okoz(df_umiod, '19')\n",
        "df_20 = preprocess_okoz(df_umiod, '20')\n",
        "df_21 = preprocess_okoz(df_umiod, '21')"
      ],
      "metadata": {
        "id": "qrzqMYOowJjp"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10,\n",
        "                    df_11, df_12, df_13, df_14, df_15, df_16, df_17, df_18, df_19,\n",
        "                    df_20, df_21], ignore_index=True)"
      ],
      "metadata": {
        "id": "26-JY0BLwkHO"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(df):\n",
        "\n",
        "  def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "\n",
        "  df['related_texts'] = df['related_texts'].apply(lambda x: ' '.join(x))\n",
        "  df['related_texts'] = df['related_texts'].apply(clean_individual_text)\n",
        "  return df"
      ],
      "metadata": {
        "id": "a5QusSRdwn79"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new = preprocess_text(df_all)"
      ],
      "metadata": {
        "id": "tGuDYK3VwqJv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_new.okoz_text.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TTP7SS4KxA8U",
        "outputId": "5a88fbbe-558d-40f9-952e-6645561e6d47"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "okoz_text\n",
              "09.00.00.00 Tadbirkorlik va xo‘jalik faoliyati                                                                                                                                                            13282\n",
              "01.00.00.00 Konstitutsiyaviy tuzum                                                                                                                                                                        10002\n",
              "07.00.00.00 Moliya va kredit to‘g‘risidagi qonunchilik. Bank faoliyati                                                                                                                                     9439\n",
              "02.00.00.00 Davlat boshqaruvi asoslari                                                                                                                                                                     7196\n",
              "21.00.00.00 O‘zgartirish va qo‘shimchalar kiritish bo‘yicha kompleks tusdagi hujjatlar                                                                                                                     6889\n",
              "03.00.00.00 Fuqarolik qonunchiligi                                                                                                                                                                         5713\n",
              "13.00.00.00 Ta’lim. Fan. Madaniyat                                                                                                                                                                         5305\n",
              "16.00.00.00 Xavfsizlik va huquq tartibot muhofazasi                                                                                                                                                        3795\n",
              "11.00.00.00 Atrof tabiiy muhit va tabiiy resurslar                                                                                                                                                         3592\n",
              "14.00.00.00 Sog‘liqni saqlash. Jismoniy tarbiya. Sport. Turizm                                                                                                                                             3041\n",
              "05.00.00.00 Mehnat va aholining bandligi to‘g‘risidagi qonunchilik                                                                                                                                         2926\n",
              "10.00.00.00 Tashqi iqtisodiy faoliyat. Bojxona ishi                                                                                                                                                        2576\n",
              "19.00.00.00 Xalqaro munosabatlar. Xalqaro huquq                                                                                                                                                            2496\n",
              "12.00.00.00 Axborot va axborotlashtirish                                                                                                                                                                   2091\n",
              "06.00.00.00 Ijtimoiy ta’minot va ijtimoiy sug‘urta to‘g‘risidagi qonunchilik. Ijtimoiy himoya                                                                                                              1819\n",
              "17.00.00.00 Odil sudlov                                                                                                                                                                                    1593\n",
              "08.00.00.00 Uy-joy qonunchiligi. Kommunal xo‘jalik                                                                                                                                                         1269\n",
              "18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari                                                                                                            1161\n",
              "15.00.00.00 Mudofaa                                                                                                                                                                                         804\n",
              "04.00.00.00 Oila qonunchiligi                                                                                                                                                                               605\n",
              "20.00.00.00 Kadrlar masalalari, mukofotlar, faxriy unvonlar, esdalik nishonlar, faxriy yorliqlar bilan taqdirlash, amnistiya va afv etish masalalari hamda fuqarolik berish bo‘yicha individual aktlar      110\n",
              "Name: count, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_embedd(df, model, tokenizer, device, batch_size=16):\n",
        "    def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    # Embed documents in batches without mixed precision\n",
        "    def embed_document(df, model, tokenizer, device, batch_size=16):\n",
        "        embeddings_list = []\n",
        "        texts = df['related_texts'].apply(clean_individual_text).tolist()\n",
        "\n",
        "        # Tokenize all the texts\n",
        "        inputs = tokenizer(texts, return_tensors='pt', truncation=True, padding=True, max_length=256)  # Adjust max_length for speed\n",
        "        dataset = TensorDataset(inputs['input_ids'], inputs['attention_mask'])\n",
        "        dataloader = DataLoader(dataset, batch_size=batch_size)\n",
        "\n",
        "        for batch in dataloader:\n",
        "            input_ids, attention_mask = [t.to(device) for t in batch]\n",
        "            with torch.no_grad():\n",
        "                outputs = model(input_ids, attention_mask=attention_mask)\n",
        "                embeddings = outputs.last_hidden_state\n",
        "                doc_embeddings = torch.mean(embeddings, dim=1)  # Take mean of token embeddings for document embedding\n",
        "\n",
        "            embeddings_list.extend(doc_embeddings.cpu().numpy())  # Move to CPU and convert to numpy\n",
        "\n",
        "        df['embeddings'] = embeddings_list\n",
        "        return df\n",
        "\n",
        "    # Convert 'okoz_text' into numeric labels\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "\n",
        "    label_to_numeric = {}\n",
        "    label_counter = 1\n",
        "\n",
        "    # Map labels based on frequency\n",
        "    for label, count in class_counts.items():\n",
        "        if count <= 1000:\n",
        "            label_to_numeric[label] = 0  # Treat less frequent labels as class 0\n",
        "        else:\n",
        "            label_to_numeric[label] = label_counter\n",
        "            label_counter += 1\n",
        "\n",
        "    df['label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    # Get embeddings\n",
        "    df = embed_document(df, model, tokenizer, device, batch_size)\n",
        "\n",
        "    # Drop unnecessary columns\n",
        "    columns_to_drop = ['okoz_text']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    return df, label_to_numeric\n",
        ""
      ],
      "metadata": {
        "id": "3XfCT3I6xDeh"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_embedded, label_to_numeric = preprocess_data_embedd(df_new, model, tokenizer, device, batch_size=64)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "id": "I_DYdD3xyTdA",
        "outputId": "acf694b1-a732-4a66-ecb1-990756ab36dc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-31-d72237c1ad5a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf_embedded\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_to_numeric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data_embedd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-30-1e7106be1a9b>\u001b[0m in \u001b[0;36mpreprocess_data_embedd\u001b[0;34m(df, model, tokenizer, device, batch_size)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;31m# Get embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membed_document\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;31m# Drop unnecessary columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-30-1e7106be1a9b>\u001b[0m in \u001b[0;36membed_document\u001b[0;34m(df, model, tokenizer, device, batch_size)\u001b[0m\n\u001b[1;32m     24\u001b[0m                 \u001b[0mdoc_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Take mean of token embeddings for document embedding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0membeddings_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdoc_embeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Move to CPU and convert to numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'embeddings'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeddings_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_model(df, batch_size):\n",
        "    X = np.array(df['embeddings'].tolist())\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "MO2gC4dsysKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = prep_model(df_embedded, 32)"
      ],
      "metadata": {
        "id": "47GHtA6Pyt59"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}