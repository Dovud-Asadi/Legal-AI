{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "FJF5HGdVJMdx"
      },
      "outputs": [],
      "source": [
        "!pip install  sentencepiece -q"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import json\n",
        "import string\n",
        "import re\n",
        "import gdown\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score\n",
        "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "from transformers import XLMRobertaTokenizer, XLMRobertaModel\n",
        "from huggingface_hub import login\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import ast"
      ],
      "metadata": {
        "id": "dQcK03AGJUXy"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "login(token = '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')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tb3zec8vJV2p",
        "outputId": "e0f6054e-b5fc-4227-f82c-abd3eaeea5f0"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_id = '12cSkbqONn4dTTqNtWJfV3QONZ_YL5Wfh'\n",
        "url = f'https://drive.google.com/uc?export=download&id={file_id}'\n",
        "\n",
        "gdown.download(url, quiet=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "zJZD3f8CJXwj",
        "outputId": "f6447bcb-e974-43bb-939c-7f8c3919f28a"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?export=download&id=12cSkbqONn4dTTqNtWJfV3QONZ_YL5Wfh\n",
            "To: /content/full_okoz.json\n",
            "100%|██████████| 9.21M/9.21M [00:00<00:00, 21.1MB/s]\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'full_okoz.json'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1dWP_krhq_jSZdxmYN4gCQXDpLfmAWN7l?usp=sharing', output='uzbek_xlm_roberta_model')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KDalDZUFJZZL",
        "outputId": "a11351bf-f3de-4bf5-c1cd-04d0b28e38c5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB config.json\n",
            "Processing file 1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU model.safetensors\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1K6y8qGq-yPU32LeWhafyj9ZRmL3CeCGB\n",
            "To: /content/uzbek_xlm_roberta_model/config.json\n",
            "100%|██████████| 709/709 [00:00<00:00, 2.40MB/s]\n",
            "Downloading...\n",
            "From (original): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU\n",
            "From (redirected): https://drive.google.com/uc?id=1v62TaVs1gWlWuyFYAC9k0QXdJq3mh-KU&confirm=t&uuid=6d946510-bbb1-4439-9193-88727cbdac42\n",
            "To: /content/uzbek_xlm_roberta_model/model.safetensors\n",
            "100%|██████████| 2.24G/2.24G [01:03<00:00, 35.5MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_model/config.json',\n",
              " 'uzbek_xlm_roberta_model/model.safetensors']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "gdown.download_folder('https://drive.google.com/drive/folders/1UDLQbCEkdS5DWKokzl-NFqxHlBZ6MHK_?usp=sharing', output='uzbek_xlm_roberta_tokenizer')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WC0pfBKVJcOO",
        "outputId": "3f83cfaa-e1d7-4dda-cd39-a31416644a79"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing file 10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG sentencepiece.bpe.model\n",
            "Processing file 1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb special_tokens_map.json\n",
            "Processing file 1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr tokenizer_config.json\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Retrieving folder contents completed\n",
            "Building directory structure\n",
            "Building directory structure completed\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=10obthWQsAOOTtr8Qx7O_qaJhQkn-upVG\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model\n",
            "100%|██████████| 5.07M/5.07M [00:00<00:00, 52.4MB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1Dao4duhJz3lGdjYMMTG1zztfXSJQWLPb\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/special_tokens_map.json\n",
            "100%|██████████| 280/280 [00:00<00:00, 974kB/s]\n",
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1wzGPT51CaZGBmGbUIW54Jf7LV6MBe0Mr\n",
            "To: /content/uzbek_xlm_roberta_tokenizer/tokenizer_config.json\n",
            "100%|██████████| 1.17k/1.17k [00:00<00:00, 4.32MB/s]\n",
            "Download completed\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['uzbek_xlm_roberta_tokenizer/sentencepiece.bpe.model',\n",
              " 'uzbek_xlm_roberta_tokenizer/special_tokens_map.json',\n",
              " 'uzbek_xlm_roberta_tokenizer/tokenizer_config.json']"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_dir = \"/content/uzbek_xlm_roberta_model\"\n",
        "tokenizer_dir = \"/content/uzbek_xlm_roberta_tokenizer\"\n",
        "\n",
        "tokenizer = XLMRobertaTokenizer.from_pretrained(tokenizer_dir)\n",
        "model = XLMRobertaModel.from_pretrained(model_dir)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "model.eval()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5flirENJeCH",
        "outputId": "59d5282a-f1b4-4369-a911-4afa7bd54939"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at /content/uzbek_xlm_roberta_model and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "XLMRobertaModel(\n",
              "  (embeddings): XLMRobertaEmbeddings(\n",
              "    (word_embeddings): Embedding(250002, 1024, padding_idx=1)\n",
              "    (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
              "    (token_type_embeddings): Embedding(1, 1024)\n",
              "    (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "    (dropout): Dropout(p=0.1, inplace=False)\n",
              "  )\n",
              "  (encoder): XLMRobertaEncoder(\n",
              "    (layer): ModuleList(\n",
              "      (0-23): 24 x XLMRobertaLayer(\n",
              "        (attention): XLMRobertaAttention(\n",
              "          (self): XLMRobertaSelfAttention(\n",
              "            (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "          (output): XLMRobertaSelfOutput(\n",
              "            (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (intermediate): XLMRobertaIntermediate(\n",
              "          (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
              "          (intermediate_act_fn): GELUActivation()\n",
              "        )\n",
              "        (output): XLMRobertaOutput(\n",
              "          (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
              "          (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
              "          (dropout): Dropout(p=0.1, inplace=False)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (pooler): XLMRobertaPooler(\n",
              "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
              "    (activation): Tanh()\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_json('/content/full_okoz.json')"
      ],
      "metadata": {
        "id": "4yOK_DR1JgVf"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_to_remove = 'Hujjatga taklif yuborish Audioni tinglash'\n",
        "\n",
        "for i in range(len(df.related_texts)):\n",
        "    for j in range(len(df.related_texts[i])):\n",
        "        df.related_texts[i][j] = df.related_texts[i][j].replace(text_to_remove, \"\")"
      ],
      "metadata": {
        "id": "WuUYmYEIJhWt"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df, model, tokenizer, device, number):\n",
        "    def contains_04_0(okoz_list):\n",
        "        return any(f'{number}.0' in item for item in okoz_list)#change\n",
        "\n",
        "    def filter_by_length(text_list):\n",
        "        return [text for text in text_list if len(text) > 4]\n",
        "\n",
        "    def keep_elements_starting_with_04(text_list):\n",
        "        return [element for element in text_list if element.startswith(f'{number}')] #change\n",
        "\n",
        "    def process_okoz_text(text_list):\n",
        "        return [text.split('/')[0].strip() if '/' in text else text for text in text_list]\n",
        "\n",
        "    def has_minimum_elements(text_list, min_length=0):\n",
        "        return len(text_list) > min_length\n",
        "\n",
        "    def preprocess_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "        return text\n",
        "\n",
        "    def remove_duplicates(text_list):\n",
        "        return list(set(text_list))\n",
        "\n",
        "    def remove_semicolons(text_list):\n",
        "        return [text.replace(';', '') for text in text_list]\n",
        "\n",
        "    def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    df = df[df['okoz_text'].apply(contains_04_0)]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(filter_by_length)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(keep_elements_starting_with_04)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(process_okoz_text)\n",
        "    df = df[df['okoz_text'].apply(lambda x: has_minimum_elements(x))]\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_duplicates)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(remove_semicolons)\n",
        "    # df = df[df['okoz_text'].apply(len) == 1]\n",
        "    df = df.reset_index(drop=True)\n",
        "    df.loc[:, 'okoz_text'] = df['okoz_text'].apply(lambda x: ' '.join(x))\n",
        "\n",
        "    df['related_texts'] = df['related_texts'].apply(lambda x: ' '.join(x))\n",
        "    df['related_texts'] = df['related_texts'].apply(clean_individual_text)\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "nrotdA1AJidb"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_1 = preprocess_data(df, model, tokenizer, device, '01')\n",
        "df_2 = preprocess_data(df, model, tokenizer, device, '02')\n",
        "df_3 = preprocess_data(df, model, tokenizer, device, '03')\n",
        "df_4 = preprocess_data(df, model, tokenizer, device, '04')\n",
        "df_5 = preprocess_data(df, model, tokenizer, device, '05')\n",
        "df_6 = preprocess_data(df, model, tokenizer, device, '06')\n",
        "df_7 = preprocess_data(df, model, tokenizer, device, '07')\n",
        "df_8 = preprocess_data(df, model, tokenizer, device, '08')\n",
        "df_9 = preprocess_data(df, model, tokenizer, device, '09')\n",
        "df_10 = preprocess_data(df, model, tokenizer, device, '10')\n",
        "df_11 = preprocess_data(df, model, tokenizer, device, '11')\n",
        "df_12 = preprocess_data(df, model, tokenizer, device, '12')\n",
        "df_13 = preprocess_data(df, model, tokenizer, device, '13')\n",
        "df_14 = preprocess_data(df, model, tokenizer, device, '14')\n",
        "df_15 = preprocess_data(df, model, tokenizer, device, '15')\n",
        "df_16 = preprocess_data(df, model, tokenizer, device, '16')\n",
        "df_17 = preprocess_data(df, model, tokenizer, device, '17')\n",
        "df_18 = preprocess_data(df, model, tokenizer, device, '18')\n",
        "df_19 = preprocess_data(df, model, tokenizer, device, '19')\n",
        "df_20 = preprocess_data(df, model, tokenizer, device, '20')\n",
        "df_21 = preprocess_data(df, model, tokenizer, device, '21')"
      ],
      "metadata": {
        "id": "fgSCaQoLJkM2"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all = pd.concat([df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10,\n",
        "                    df_11, df_12, df_13, df_14, df_15, df_16, df_17, df_18, df_19,\n",
        "                    df_20, df_21], ignore_index=True)"
      ],
      "metadata": {
        "id": "d7l28040KUml"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_all.okoz_text.value_counts()/len(df_all)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 773
        },
        "id": "MekILamHKdtD",
        "outputId": "697efc96-1d1a-4e38-f654-ac3afa2d11f6"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "okoz_text\n",
              "04.00.00.00 Oila qonunchiligi                                                                                                                                                                             0.211037\n",
              "03.00.00.00 Fuqarolik qonunchiligi                                                                                                                                                                        0.180183\n",
              "01.00.00.00 Konstitutsiyaviy tuzum                                                                                                                                                                        0.164316\n",
              "16.00.00.00 Xavfsizlik va huquq tartibot muhofazasi                                                                                                                                                       0.066467\n",
              "02.00.00.00 Davlat boshqaruvi asoslari                                                                                                                                                                    0.065233\n",
              "17.00.00.00 Odil sudlov                                                                                                                                                                                   0.040197\n",
              "18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari                                                                                                           0.040021\n",
              "19.00.00.00 Xalqaro munosabatlar. Xalqaro huquq                                                                                                                                                           0.032616\n",
              "06.00.00.00 Ijtimoiy ta’minot va ijtimoiy sug‘urta to‘g‘risidagi qonunchilik. Ijtimoiy himoya                                                                                                             0.030853\n",
              "14.00.00.00 Sog‘liqni saqlash. Jismoniy tarbiya. Sport. Turizm                                                                                                                                            0.024506\n",
              "09.00.00.00 Tadbirkorlik va xo‘jalik faoliyati                                                                                                                                                            0.023977\n",
              "07.00.00.00 Moliya va kredit to‘g‘risidagi qonunchilik. Bank faoliyati                                                                                                                                    0.023977\n",
              "13.00.00.00 Ta’lim. Fan. Madaniyat                                                                                                                                                                        0.019746\n",
              "12.00.00.00 Axborot va axborotlashtirish                                                                                                                                                                  0.017630\n",
              "05.00.00.00 Mehnat va aholining bandligi to‘g‘risidagi qonunchilik                                                                                                                                        0.013752\n",
              "08.00.00.00 Uy-joy qonunchiligi. Kommunal xo‘jalik                                                                                                                                                        0.012341\n",
              "21.00.00.00 O‘zgartirish va qo‘shimchalar kiritish bo‘yicha kompleks tusdagi hujjatlar                                                                                                                    0.010755\n",
              "11.00.00.00 Atrof tabiiy muhit va tabiiy resurslar                                                                                                                                                        0.008992\n",
              "15.00.00.00 Mudofaa                                                                                                                                                                                       0.007581\n",
              "20.00.00.00 Kadrlar masalalari, mukofotlar, faxriy unvonlar, esdalik nishonlar, faxriy yorliqlar bilan taqdirlash, amnistiya va afv etish masalalari hamda fuqarolik berish bo‘yicha individual aktlar    0.004231\n",
              "10.00.00.00 Tashqi iqtisodiy faoliyat. Bojxona ishi                                                                                                                                                       0.001587\n",
              "Name: count, dtype: float64"
            ],
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>count</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>okoz_text</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>04.00.00.00 Oila qonunchiligi</th>\n",
              "      <td>0.211037</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>03.00.00.00 Fuqarolik qonunchiligi</th>\n",
              "      <td>0.180183</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>01.00.00.00 Konstitutsiyaviy tuzum</th>\n",
              "      <td>0.164316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16.00.00.00 Xavfsizlik va huquq tartibot muhofazasi</th>\n",
              "      <td>0.066467</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>02.00.00.00 Davlat boshqaruvi asoslari</th>\n",
              "      <td>0.065233</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17.00.00.00 Odil sudlov</th>\n",
              "      <td>0.040197</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18.00.00.00 Prokuratura. Advokatura. Notariat. Yuridik xizmat. Adliya organlari. FHDY organlari</th>\n",
              "      <td>0.040021</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19.00.00.00 Xalqaro munosabatlar. Xalqaro huquq</th>\n",
              "      <td>0.032616</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>06.00.00.00 Ijtimoiy ta’minot va ijtimoiy sug‘urta to‘g‘risidagi qonunchilik. Ijtimoiy himoya</th>\n",
              "      <td>0.030853</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14.00.00.00 Sog‘liqni saqlash. Jismoniy tarbiya. Sport. Turizm</th>\n",
              "      <td>0.024506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>09.00.00.00 Tadbirkorlik va xo‘jalik faoliyati</th>\n",
              "      <td>0.023977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>07.00.00.00 Moliya va kredit to‘g‘risidagi qonunchilik. Bank faoliyati</th>\n",
              "      <td>0.023977</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13.00.00.00 Ta’lim. Fan. Madaniyat</th>\n",
              "      <td>0.019746</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12.00.00.00 Axborot va axborotlashtirish</th>\n",
              "      <td>0.017630</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>05.00.00.00 Mehnat va aholining bandligi to‘g‘risidagi qonunchilik</th>\n",
              "      <td>0.013752</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>08.00.00.00 Uy-joy qonunchiligi. Kommunal xo‘jalik</th>\n",
              "      <td>0.012341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21.00.00.00 O‘zgartirish va qo‘shimchalar kiritish bo‘yicha kompleks tusdagi hujjatlar</th>\n",
              "      <td>0.010755</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11.00.00.00 Atrof tabiiy muhit va tabiiy resurslar</th>\n",
              "      <td>0.008992</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15.00.00.00 Mudofaa</th>\n",
              "      <td>0.007581</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20.00.00.00 Kadrlar masalalari, mukofotlar, faxriy unvonlar, esdalik nishonlar, faxriy yorliqlar bilan taqdirlash, amnistiya va afv etish masalalari hamda fuqarolik berish bo‘yicha individual aktlar</th>\n",
              "      <td>0.004231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10.00.00.00 Tashqi iqtisodiy faoliyat. Bojxona ishi</th>\n",
              "      <td>0.001587</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div><br><label><b>dtype:</b> float64</label>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data_embedd(df, model, tokenizer, device):\n",
        "    def clean_individual_text(text):\n",
        "        text = text.lower()\n",
        "        text = text.replace('‘', \"'\").replace('’', \"'\").replace('`', \"'\")\n",
        "        text = re.sub(r'[^a-z\\.\\'\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def embed_document(df, model, tokenizer, device):\n",
        "        embeddings_list = []\n",
        "\n",
        "        for i in range(len(df['related_texts'])):\n",
        "            cleaned_text = clean_individual_text(df['related_texts'][i])\n",
        "\n",
        "            inputs = tokenizer(cleaned_text, return_tensors='pt', truncation=True, padding=True)\n",
        "            inputs = {key: value.to(device) for key, value in inputs.items()}\n",
        "\n",
        "            with torch.no_grad():\n",
        "                outputs = model(**inputs)\n",
        "\n",
        "            embeddings = outputs.last_hidden_state\n",
        "            doc_embedding = torch.mean(embeddings, dim=1).squeeze()\n",
        "\n",
        "            embeddings_list.append(doc_embedding.cpu().numpy())\n",
        "\n",
        "        df['embeddings'] = embeddings_list\n",
        "\n",
        "        return df\n",
        "\n",
        "    class_counts = df['okoz_text'].value_counts()\n",
        "\n",
        "    label_to_numeric = {}\n",
        "    label_counter = 1\n",
        "\n",
        "    for label, count in class_counts.items():\n",
        "      if count <=100:\n",
        "        label_to_numeric[label] = 0\n",
        "      else:\n",
        "        label_to_numeric[label] = label_counter\n",
        "        label_counter += 1\n",
        "\n",
        "    df.loc[:, 'label'] = df['okoz_text'].map(label_to_numeric)\n",
        "\n",
        "    df = embed_document(df, model, tokenizer, device)\n",
        "\n",
        "    columns_to_drop = ['okoz_text']\n",
        "    df = df.drop(columns=columns_to_drop)\n",
        "\n",
        "    return df, label_to_numeric"
      ],
      "metadata": {
        "id": "XLkbE-XQKgnz"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df, label_to_numeric = preprocess_data_embedd(df_all, model, tokenizer, device)"
      ],
      "metadata": {
        "id": "nJPubPkSKh0d"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def prep_model(df, batch_size):\n",
        "    X = np.array(df['embeddings'].tolist())\n",
        "    y = np.array(df['label'])\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state=42, stratify=y)\n",
        "\n",
        "    train_dataset = TensorDataset(torch.tensor(X_train, dtype=torch.float32), torch.tensor(y_train, dtype=torch.long))\n",
        "    test_dataset = TensorDataset(torch.tensor(X_test, dtype=torch.float32), torch.tensor(y_test, dtype=torch.long))\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    return train_loader, test_loader"
      ],
      "metadata": {
        "id": "rRV6JogYKifP"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, test_loader = prep_model(df, 32)"
      ],
      "metadata": {
        "id": "UOVrV96iKk4Y"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "num_classes = df['label'].nunique()\n",
        "input_dim = np.array(df['embeddings'][0]).shape[0]"
      ],
      "metadata": {
        "id": "GaJzOBZdKpw0"
      },
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "tiseovcaKq9A"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(train_loader, model, criterion, optimizer, scheduler, num_epochs=100):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            # Print shapes for debugging\n",
        "            print(\"Output shape:\", outputs.shape)\n",
        "            print(\"Labels shape:\", labels.shape)\n",
        "\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        scheduler.step(epoch_loss)\n",
        "        if epoch % 10 == 0:\n",
        "            print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.6f}\")\n",
        "\n",
        "    print(\"Training Complete\")"
      ],
      "metadata": {
        "id": "i3ANxYf8MfPd"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_model(model, test_loader, criterion, df):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    total_loss = 0.0\n",
        "    incorrect_predictions = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i, (inputs, labels) in enumerate(test_loader):\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "            incorrect_indices = (predicted != labels).nonzero(as_tuple=True)[0]\n",
        "            for idx in incorrect_indices:\n",
        "                original_idx = i * test_loader.batch_size + idx.item()\n",
        "                text = df.iloc[original_idx]['related_texts']\n",
        "                true_label = labels[idx].item()\n",
        "                predicted_label = predicted[idx].item()\n",
        "                incorrect_predictions.append((text, true_label, predicted_label))\n",
        "\n",
        "    accuracy = 100 * correct / total\n",
        "    avg_loss = total_loss / total\n",
        "    print(f\"Test Accuracy: {accuracy:.2f}%, Test Loss: {avg_loss:.4f}\")\n",
        "\n",
        "    if incorrect_predictions:\n",
        "        print(\"\\nIncorrect Predictions:\")\n",
        "        for text, true_label, predicted_label in incorrect_predictions:\n",
        "            print(f\"Text: {text}\")\n",
        "            print(f\"True Label: {true_label}, Predicted Label: {predicted_label}\\n\")\n",
        "\n",
        "    return accuracy, avg_loss, incorrect_predictions"
      ],
      "metadata": {
        "id": "ZFbfN3VaKskA"
      },
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MulticlassModel1(nn.Module):\n",
        "    def __init__(self, input_size, num_classes):\n",
        "        super(MulticlassModel1, self).__init__()\n",
        "        self.layer1 = nn.Linear(input_size, 512)\n",
        "        self.layer2 = nn.Linear(512, 1024)\n",
        "        self.layer3 = nn.Linear(1024, 512)\n",
        "        self.output = nn.Linear(512, num_classes)\n",
        "\n",
        "        self.relu = nn.ReLU()\n",
        "        self.dropout = nn.Dropout(p=0.4)\n",
        "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
        "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.relu(self.batch_norm1(self.layer1(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.batch_norm2(self.layer2(x)))\n",
        "        x = self.dropout(x)\n",
        "        x = self.relu(self.layer3(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.output(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "RDp4t54KKvK_"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = MulticlassModel1(input_size=input_dim, num_classes=num_classes).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.AdamW(model.parameters(), lr=0.001, weight_decay=1e-5)\n",
        "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "dGbdjXZuKx04",
        "outputId": "53aa9801-c47e-4c0b-9779-5bfcac719bae"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-58-6eebb3dd8b88>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMulticlassModel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdamW\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mscheduler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReduceLROnPlateau\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'min'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfactor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1172\u001b[0m                     \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1174\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1176\u001b[0m     def register_full_backward_pre_hook(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    778\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrecurse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    779\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 780\u001b[0;31m                 \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    781\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    782\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn, recurse)\u001b[0m\n\u001b[1;32m    803\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    804\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 805\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    806\u001b[0m             \u001b[0mp_should_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    807\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1158\u001b[0m                         \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_to_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1159\u001b[0m                     )\n\u001b[0;32m-> 1160\u001b[0;31m                 return t.to(\n\u001b[0m\u001b[1;32m   1161\u001b[0m                     \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1162\u001b[0m                     \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# In your data loader definition\n",
        "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True) # Reduced batch size"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "PZdYKsL0Miar",
        "outputId": "8bc4e1fc-8527-4db4-e0cb-31517a890f9c"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'train_dataset' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-30-70ae242e325a>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# In your data loader definition\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mtrain_loader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDataLoader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Reduced batch size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_model(train_loader, model, criterion, optimizer,scheduler, num_epochs=100)\n",
        "accuracy, avg_loss, incorrect_predictions = evaluate_model(model, test_loader, criterion, df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 372
        },
        "id": "U3DjNBNRKzD1",
        "outputId": "03da567e-b1d7-4760-bf1c-c03ad82997f0"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-88117be8682d>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0maccuracy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mavg_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mincorrect_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-23-6fefaf20aa18>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_loader, model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m      8\u001b[0m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m             \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    519\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m             )\n\u001b[0;32m--> 521\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_tensor_or_tensors_to_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 282\u001b[0;31m     \u001b[0mgrad_tensors_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_make_grads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mis_grads_batched\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    283\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mretain_graph\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    284\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36m_make_grads\u001b[0;34m(outputs, grads, is_grads_batched)\u001b[0m\n\u001b[1;32m    159\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m                 new_grads.append(\n\u001b[0;32m--> 161\u001b[0;31m                     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreserve_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    162\u001b[0m                 )\n\u001b[1;32m    163\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: CUDA error: device-side assert triggered\nCUDA kernel errors might be asynchronously reported at some other API call, so the stacktrace below might be incorrect.\nFor debugging consider passing CUDA_LAUNCH_BLOCKING=1\nCompile with `TORCH_USE_CUDA_DSA` to enable device-side assertions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHsNNq2LMJhD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}